{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q-1-3.py\n",
    "\n",
    "Contrast the effectiveness of Misclassification rate, Gini, Entropy as impurity measures in terms of precision, recall and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree, preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "eps = np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input_data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spliting features and class label and dropping numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df.left\n",
    "X = df.drop(['left'],axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doing One hot Encoding to convert multi-valued features to binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = pd.concat([X,pd.get_dummies(X['sales'], prefix='sales')],axis='columns')\n",
    "Z = pd.concat([Z,pd.get_dummies(Z['salary'], prefix='salary')],axis='columns')\n",
    "Z = Z.drop(['sales','salary'],axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(Z, Y,test_size=0.2)\n",
    "df_ent = pd.concat([X_train,Y_train],axis=1)\n",
    "df_gini = pd.concat([X_train,Y_train],axis=1)\n",
    "df_mis_rate = pd.concat([X_train,Y_train],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mymodel function takes two argumetns : \n",
    "1 : dataframe\n",
    "2 : flag\n",
    "```\n",
    "    flag == 1 : use entropy  as impurity measure\n",
    "    flag == 2 : use gini index as impurity measure\n",
    "    flag == 3 : use misclassification rate as impurity measure\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymodel(df1, flag ):\n",
    "    def get_impurity( df ):\n",
    "        Class = df.keys()[-1]\n",
    "        if flag == 1:\n",
    "            entropy = 0\n",
    "            values = df[Class].unique()\n",
    "            for value in values:\n",
    "                fraction = float(df[Class].value_counts()[value])/len(df[Class])\n",
    "                entropy += -fraction*np.log2(fraction+eps)\n",
    "            return entropy\n",
    "        if flag == 2:\n",
    "            entropy = 1\n",
    "            values = df[Class].unique()\n",
    "            for value in values:\n",
    "                fraction = float(df[Class].value_counts()[value])/len(df[Class])\n",
    "                entropy *= fraction\n",
    "            return 2*entropy\n",
    "        if flag == 3:\n",
    "            entropy = 1\n",
    "            values = df[Class].unique()\n",
    "            for value in values:\n",
    "                fraction = float(df[Class].value_counts()[value])/len(df[Class])\n",
    "                entropy = min(fraction , 1-fraction)\n",
    "            return entropy\n",
    "\n",
    "    def get_impurity_attr( df,  attribute ):\n",
    "        if flag == 1:\n",
    "            Class = df.keys()[-1]\n",
    "            target_variables = df[Class].unique()  \n",
    "            variables = df[attribute].unique()\n",
    "            entropy2 = 0\n",
    "            for variable in variables:\n",
    "                entropy = 0\n",
    "                for target_variable in target_variables:\n",
    "                    num = len(df[attribute][df[attribute]==variable][df[Class] ==target_variable])\n",
    "                    den = len(df[attribute][df[attribute]==variable])\n",
    "                    fraction = num/(den+eps)\n",
    "                    entropy += -fraction*np.log2(fraction+eps)\n",
    "                fraction2 = float(den)/len(df)\n",
    "                entropy2 += -fraction2*entropy\n",
    "            return abs(entropy2)\n",
    "        if flag == 2:\n",
    "            Class = df.keys()[-1]\n",
    "            target_variables = df[Class].unique()  \n",
    "            variables = df[attribute].unique()\n",
    "            entropy2 = 0\n",
    "            for variable in variables:\n",
    "                entropy = 1\n",
    "                for target_variable in target_variables:\n",
    "                    num = len(df[attribute][df[attribute]==variable][df[Class] ==target_variable])\n",
    "                    den = len(df[attribute][df[attribute]==variable])\n",
    "                    fraction = num/(den+eps)\n",
    "                    entropy *= fraction\n",
    "                entropy *= 2\n",
    "                fraction2 = float(den)/len(df)\n",
    "                entropy2 += fraction2*entropy\n",
    "            return entropy2\n",
    "        if flag == 3:\n",
    "            Class = df.keys()[-1]\n",
    "            target_variables = df[Class].unique()  \n",
    "            variables = df[attribute].unique()\n",
    "            entropy2 = 0\n",
    "            for variable in variables:\n",
    "                entropy = 1\n",
    "                for target_variable in target_variables:\n",
    "                    num = len(df[attribute][df[attribute]==variable][df[Class] ==target_variable])\n",
    "                    den = len(df[attribute][df[attribute]==variable])\n",
    "                    fraction = num/(den+eps)\n",
    "                    entropy = min(fraction, 1 - fraction)\n",
    "                fraction2 = float(den)/len(df)\n",
    "                entropy2 += fraction2*entropy\n",
    "            return entropy2\n",
    "\n",
    "    def get_subtable(df, node, value):\n",
    "        return df[df[node] == value].reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    def find_IG( df, val, attr  ):\n",
    "        class_ent = get_impurity(df)\n",
    "        left = df[df[attr] < val ].reset_index(drop=True)\n",
    "        right = df[df[attr] >= val ].reset_index(drop=True)\n",
    "        left_imp = get_impurity(left)\n",
    "        right_imp = get_impurity(right)\n",
    "        return class_ent - ((float(len(left))/(len(df)+eps) * left_imp)+( float(len(right))/(len(df)+eps) * right_imp))\n",
    "\n",
    "    num_attrs = [ 'number_project','last_evaluation', 'satisfaction_level','average_montly_hours','time_spend_company']\n",
    "    split_dict = {}\n",
    "    \n",
    "    def split_numerical( attr , Y , attr_name ):\n",
    "        max_ig = 0\n",
    "        max_split = None\n",
    "        pair = pd.concat([attr, Y], axis='columns')\n",
    "        pair = pair.sort_values(by =attr_name).reset_index()\n",
    "        myset = set()\n",
    "        for i in xrange( len(attr)-1):\n",
    "            if pair['left'][i] != pair['left'][i+1] and (float(pair[attr_name][i] + pair[attr_name][i+1])/2) not in myset:\n",
    "                myset.add(float(pair[attr_name][i] + pair[attr_name][i+1])/2)\n",
    "                cur_ig = find_IG( pair, float(pair[attr_name][i] + pair[attr_name][i+1])/2 , attr_name )\n",
    "                if cur_ig > max_ig:\n",
    "                    max_ig = cur_ig\n",
    "                    max_split =  float(pair[attr_name][i] + pair[attr_name][i+1])/2\n",
    "        return max_split\n",
    "\n",
    "    def attr_to_select(df):\n",
    "        num_attr = [ i for i in df.columns if i in num_attrs]\n",
    "        for attr in num_attr:\n",
    "            split_val = split_numerical(df[attr], df['left'],attr)\n",
    "            split_dict[attr] = split_val\n",
    "        IG = []\n",
    "        for key in df.keys()[:-1]:\n",
    "            IG.append(get_impurity(df)-get_impurity_attr(df,key))\n",
    "        return df.keys()[:-1][np.argmax(IG)]\n",
    "\n",
    "\n",
    "    class Node:\n",
    "        def __init__(self, feature):\n",
    "            self.feature = feature\n",
    "            self.split_val = 0\n",
    "            self.positive = 0\n",
    "            self.negative = 0\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "\n",
    "    def build_Tree(df):\n",
    "        if len(df.columns) == 1:\n",
    "            return None\n",
    "        node_to_split = attr_to_select(df)\n",
    "\n",
    "        root = Node(node_to_split)\n",
    "        subtable0 = []\n",
    "        subtable1 = []\n",
    "        if node_to_split in num_attrs:\n",
    "            split_point = split_dict[node_to_split]\n",
    "\n",
    "            root.split_val = split_point\n",
    "            root.positive = len( df[df['left'] >= split_point]['left'] )\n",
    "            root.negative = len( df[df['left'] <  split_point]['left'] )\n",
    "\n",
    "            subtable0 = df[df[node_to_split] < split_point].reset_index(drop=True)\n",
    "            subtable1 = df[df[node_to_split] >= split_point].reset_index(drop=True)\n",
    "\n",
    "        else:\n",
    "            root.positive = len( df[df['left']==1]['left'] )\n",
    "            root.negative = len( df[df['left']==0]['left'] )\n",
    "\n",
    "            subtable0 = get_subtable(df,node_to_split,0)\n",
    "            subtable1 = get_subtable(df,node_to_split,1)\n",
    "\n",
    "\n",
    "        subtable0 = subtable0.drop(node_to_split,axis=1)\n",
    "        subtable1 = subtable1.drop(node_to_split,axis=1)\n",
    "\n",
    "        clValue0,counts0 = np.unique(subtable0['left'],return_counts=True)\n",
    "        clValue1,counts1 = np.unique(subtable1['left'],return_counts=True)\n",
    "\n",
    "        if len(counts0)>1:\n",
    "            root.left = build_Tree(subtable0)\n",
    "        if len(counts1)>1:\n",
    "            root.right = build_Tree(subtable1)\n",
    "\n",
    "        return root\n",
    "\n",
    "    def rec_predict(df,root,Y1):\n",
    "        if root == None:\n",
    "            return None\n",
    "\n",
    "        if root.feature in num_attrs:\n",
    "            try:\n",
    "                if root.right==None and root.left==None:\n",
    "                    Y1.append(1 if root.positive > root.negative else 0)\n",
    "                    return\n",
    "\n",
    "                if root.right==None and df[root.feature] >= root.split_val:\n",
    "                    Y1.append(1 if root.positive > root.negative else 0)\n",
    "                    return \n",
    "                if root.left == None and df[root.feature] < root.split_val:\n",
    "                    Y1.append(1 if root.positive > root.negative else 0)\n",
    "                    return\n",
    "\n",
    "                if df[root.feature]< root.split_val:\n",
    "                    rec_predict(df,root.left,Y1)\n",
    "                else:\n",
    "                    rec_predict(df,root.right,Y1)\n",
    "            except KeyError:\n",
    "                if root.left == None:\n",
    "                    Y1.append(1 if root.positive > root.negative else 0)\n",
    "                    return\n",
    "                rec_predict(df,root.left,Y1)\n",
    "        else:\n",
    "            try:\n",
    "                if root.right==None and root.left==None:\n",
    "                    Y1.append(1 if root.positive > root.negative else 0)\n",
    "                    return\n",
    "\n",
    "                if root.right==None and df[root.feature] == 1:\n",
    "                    Y1.append(1 if root.positive > root.negative else 0)\n",
    "                    return \n",
    "                if root.left == None and df[root.feature] == 0:\n",
    "                    Y1.append(1 if root.positive > root.negative else 0)\n",
    "                    return\n",
    "\n",
    "                if df[root.feature]==0:\n",
    "                    rec_predict(df,root.left,Y1)\n",
    "                else:\n",
    "                    rec_predict(df,root.right,Y1)\n",
    "            except KeyError:\n",
    "                if root.left == None:\n",
    "                    Y1.append(1 if root.positive > root.negative else 0)\n",
    "                    return\n",
    "                rec_predict(df,root.left,Y1)\n",
    "\n",
    "    def predict(df,root,Y1):\n",
    "        for col,row in df.iterrows():\n",
    "            rec_predict(row,root,Y1)\n",
    "\n",
    "    root = build_Tree(df1)\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    X_test_copy = X_test.copy(deep=True)\n",
    "    Yp=[]\n",
    "    predict(X_test_copy,root,Yp)\n",
    "\n",
    "    print confusion_matrix(Y_test,Yp)\n",
    "    print classification_report(Y_test,Yp)\n",
    "    print accuracy_score(Y_test, Yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1685   30]\n",
      " [  50  483]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      1715\n",
      "           1       0.94      0.91      0.92       533\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2248\n",
      "   macro avg       0.96      0.94      0.95      2248\n",
      "weighted avg       0.96      0.96      0.96      2248\n",
      "\n",
      "0.9644128113879004\n"
     ]
    }
   ],
   "source": [
    "mymodel(df_ent, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1679   36]\n",
      " [  47  486]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      1715\n",
      "           1       0.93      0.91      0.92       533\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2248\n",
      "   macro avg       0.95      0.95      0.95      2248\n",
      "weighted avg       0.96      0.96      0.96      2248\n",
      "\n",
      "0.9630782918149466\n"
     ]
    }
   ],
   "source": [
    "mymodel(df_gini, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1696   19]\n",
      " [ 169  364]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.99      0.95      1715\n",
      "           1       0.95      0.68      0.79       533\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      2248\n",
      "   macro avg       0.93      0.84      0.87      2248\n",
      "weighted avg       0.92      0.92      0.91      2248\n",
      "\n",
      "0.9163701067615658\n"
     ]
    }
   ],
   "source": [
    "mymodel(df_mis_rate, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "1. Impurity measures impacts performace a lot\n",
    "2. Gini and entropy performed same for most cases\n",
    "3. Gini is easier to compute than entropy\n",
    "4. misclassfication rate is worst performer among the three is computation is way simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
